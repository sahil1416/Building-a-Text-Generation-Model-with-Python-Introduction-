{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611e5e58-7d7d-41d9-8d99-d68cd8850f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sahil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/sahil/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/sahil/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████| 2/2 [00:01<00:00,  1.80it/s]\n",
      "Evaluating: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 21.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 6.7452, Val Loss: 6.5485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████| 2/2 [00:00<00:00,  2.24it/s]\n",
      "Evaluating: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 18.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 6.3171, Val Loss: 6.4051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████| 2/2 [00:01<00:00,  1.93it/s]\n",
      "Evaluating: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 22.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 6.0324, Val Loss: 6.4844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████| 2/2 [00:00<00:00,  2.28it/s]\n",
      "Evaluating: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 20.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 5.9560, Val Loss: 6.6235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████| 2/2 [00:00<00:00,  2.50it/s]\n",
      "Evaluating: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 22.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 6.0139, Val Loss: 6.7400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████| 2/2 [00:00<00:00,  2.32it/s]\n",
      "Evaluating: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 22.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 5.9990, Val Loss: 6.7987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████| 2/2 [00:00<00:00,  2.58it/s]\n",
      "Evaluating: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 22.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Loss: 5.9289, Val Loss: 6.8539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████| 2/2 [00:00<00:00,  2.50it/s]\n",
      "Evaluating: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 24.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Loss: 5.9455, Val Loss: 6.9255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████| 2/2 [00:00<00:00,  2.50it/s]\n",
      "Evaluating: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 21.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Loss: 5.9754, Val Loss: 6.9821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████| 2/2 [00:00<00:00,  2.52it/s]\n",
      "Evaluating: 100%|█████████████████████████████████| 1/1 [00:00<00:00, 21.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Loss: 5.9406, Val Loss: 7.0219\n",
      "Training completed!\n",
      "\n",
      "Entering text generation mode...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or 'quit' to exit):  this is legal\n",
      "Enter temperature (0.1-1.0, higher for more randomness):  100\n",
      "Enter number of tokens to generate:  100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text:\n",
      " legal feature generate body containing similar concern mode changed provided limiting foundation expected one article generally licensors perpetuity httpswwwgnuorglicenses hypothetical object acknowledges licensed general actual regenerate safest area licensing generate offer unpacking line alleging menu particular cure remove exclusively local following notice enables information subdividing authorization entered prohibit occurs used programming likewise provisionally product personal implementation public prove propagation subject implement simultaneously prohibit copyrightlike unless transaction imposed imposed us single kind server correction original counterclaim medium regenerate appropriate operation copyright year visible differ accessible inability owned manner intimate assures accord must generalpurpose script added reviewing occurring feature saying conditioned termination correction\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or 'quit' to exit):  this is legal \n",
      "Enter temperature (0.1-1.0, higher for more randomness):  1\n",
      "Enter number of tokens to generate:  100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text:\n",
      " legal arrangement cease work section work conveying revised work advised term work permanently version patent patent normal apply copy may copyright code copy explicitly circumstance third license explicitly code server copyright continue may copy c license part technological give party object price propagate agreement transaction covered apply work component relying order license material nothing licensing sale modified work used includes available must additional including copy given component liability apply violation permitted foundation purpose server permanently entity work warranty commitment convey convey license appropriate sale entire use freedom available working network shall added used reinstated propagation license component source patent gnu copyright\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or 'quit' to exit):  this is legal\n",
      "Enter temperature (0.1-1.0, higher for more randomness):  0.1\n",
      "Enter number of tokens to generate:  100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text:\n",
      " legal license license license work work license license work license work work work work work work license work work license work work work work license work work work license work license work work work license work work license work work work license work license work work work work work work work work work work work work work work work work license license work work work license license work work work work license work work work work work work work work work work license work work work work work license license work work work work license work work work work work license\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or 'quit' to exit):  this is legal\n",
      "Enter temperature (0.1-1.0, higher for more randomness):  1000\n",
      "Enter number of tokens to generate:  100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text:\n",
      " legal obligated 3 criterion sale acquired offered remain offer sold reviewing programto inclusion prove exclusion distribute knowledge agreement occurring away limit indemnification publicly operated arrange away effect although enable 0 impose copyrightlike could sign relicensing paper incompatible modifying given copyrightlike item price display notify damage long published obligate incorporating notwithstanding support run relevant unpacking acceptance thus inability show portion legal version script system foundation affirmed earlier plus exclusively adopted disclaimer family prominent electronic practice importing semiconductor pertinent consequence obligated procuring corresponding asset writing widely might ready add next downstream crossclaim dwelling author useful enable control relicensing systematic approximates applied render merchantability\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or 'quit' to exit):  this is legal\n",
      "Enter temperature (0.1-1.0, higher for more randomness):  10000\n",
      "Enter number of tokens to generate:  100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text:\n",
      " legal liable rendered terminates develop advised way obligation implied consequence 11 constantly threatened modified right interpretation show responsible return saying neither operating liable paper affirmed ability enforcing copying programto interpretation using feature copyrighted determining worldwide invalidate installed protect propagating stated place choose keep display may distributed anything obligated performing manner incorporation covered published copy lesser interface expects prohibits satisfy first actual library conveying perpetuity nonexercise andor programming december abuse charge inability others transferred mere force invalidate full infringed medium danger qualify welcome development keep could part litigation link prohibits fee copying threatened covenant downstream local class greatest like consumer program particular\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def load_data():\n",
    "    dataset = '/Users/sahil/My Data/Assignment'\n",
    "    corpus = []\n",
    "    try:\n",
    "        for filename in os.listdir(dataset):\n",
    "            if filename.endswith('.txt'):\n",
    "                with open(os.path.join(dataset, filename), 'r', encoding='utf-8') as file:\n",
    "                    corpus.append(file.read())\n",
    "        print(f\"Loaded {len(corpus)} documents.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"The specified directory was not found. Please check the path and try again.\")\n",
    "    return corpus\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation.replace(' ', '')))\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words or token == ' ']\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) if token != ' ' else token for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, corpus, sequence_length):\n",
    "        self.corpus = corpus\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab = sorted(set(token for doc in corpus for token in doc))\n",
    "        self.token2idx = {token: idx for idx, token in enumerate(self.vocab)}\n",
    "        self.idx2token = {idx: token for token, idx in self.token2idx.items()}\n",
    "        self.data = self.prepare_data()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        data = []\n",
    "        for doc in self.corpus:\n",
    "            indices = [self.token2idx[token] for token in doc]\n",
    "            for i in range(0, len(indices) - self.sequence_length, self.sequence_length):\n",
    "                chunk = indices[i:i + self.sequence_length]\n",
    "                target = indices[i + 1:i + self.sequence_length + 1]\n",
    "                if len(chunk) == self.sequence_length and len(target) == self.sequence_length:\n",
    "                    data.append((chunk, target))\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx][0]), torch.tensor(self.data[idx][1])\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, dim_feedforward, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
    "        self.d_model = d_model\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.embedding(src) * np.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        return self.fc_out(output)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in tqdm(train_loader, desc=\"Training\"):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src)\n",
    "        loss = criterion(output.view(-1, output.size(-1)), tgt.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            output = model(src)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), tgt.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "import torch\n",
    "\n",
    "def nucleus_sample(predictions, p=0.9):\n",
    "    sorted_probs, sorted_indices = torch.sort(predictions, descending=True)\n",
    "    cumulative_probs = torch.cumsum(torch.softmax(sorted_probs, dim=-1), dim=-1)\n",
    "    sorted_indices_to_remove = cumulative_probs > p\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "    indices_to_remove = torch.zeros_like(predictions, dtype=torch.bool).scatter_(\n",
    "        dim=-1, index=sorted_indices, src=sorted_indices_to_remove\n",
    "    )\n",
    "    predictions[indices_to_remove] = float('-inf')\n",
    "    \n",
    "    return torch.multinomial(torch.softmax(predictions, dim=-1), num_samples=1).item()\n",
    "\n",
    "def generate_text(model, dataset, start_tokens, num_generate=50, temperature=1.0, top_p=0.9, device='cpu'):\n",
    "    model.eval()\n",
    "    input_eval = torch.tensor([[dataset.token2idx[token] for token in start_tokens]], dtype=torch.long).to(device)\n",
    "    generated_tokens = start_tokens.copy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_generate):\n",
    "            output = model(input_eval)\n",
    "            predictions = output[0, -1, :] / temperature\n",
    "            predicted_id = nucleus_sample(predictions, p=top_p)\n",
    "\n",
    "            predicted_token = dataset.idx2token[predicted_id]\n",
    "            generated_tokens.append(predicted_token)\n",
    "\n",
    "            input_eval = torch.cat([input_eval[:, 1:], torch.tensor([[predicted_id]], device=device)], dim=1)\n",
    "\n",
    "    return ' '.join(generated_tokens)\n",
    "    \n",
    "def generate_from_prompt(model, dataset, device):\n",
    "    while True:\n",
    "        prompt = input(\"Enter a prompt (or 'quit' to exit): \")\n",
    "        if prompt.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        start_tokens = preprocess_text(prompt)[:dataset.sequence_length]\n",
    "        temperature = float(input(\"Enter temperature (0.1-1.0, higher for more randomness): \"))\n",
    "        num_tokens = int(input(\"Enter number of tokens to generate: \"))\n",
    "        \n",
    "        generated_text = generate_text(model, dataset, start_tokens, num_generate=num_tokens, temperature=temperature, device=device)\n",
    "        print(\"\\nGenerated text:\\n\", generated_text)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    corpus = load_data()\n",
    "    processed_corpus = [preprocess_text(text) for text in corpus]\n",
    "    \n",
    "    # Create dataset\n",
    "    sequence_length = 50  # You can adjust this value\n",
    "    dataset = TextDataset(processed_corpus, sequence_length)\n",
    "    \n",
    "    # Split dataset\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    # Create data loaders\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model\n",
    "    vocab_size = len(dataset.token2idx)\n",
    "    d_model = 512\n",
    "    nhead = 8\n",
    "    num_encoder_layers = 6\n",
    "    dim_feedforward = 2048\n",
    "    dropout = 0.1\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = TransformerModel(vocab_size, d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Training loop\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epochs = 10\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = evaluate(model, val_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    # Text generation\n",
    "    print(\"\\nEntering text generation mode...\")\n",
    "    generate_from_prompt(model, dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff94e97-70fe-486a-8e5b-a7f5f932bd50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25ee22df-498b-47aa-b986-063b5632bf83",
   "metadata": {},
   "source": [
    "# Quantitative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8214c819-6331-4f64-a1b1-a96b022d1c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class YourTestDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return input and target for the idx-th item\n",
    "        return self.data[idx]['input'], self.data[idx]['target']\n",
    "\n",
    "# Prepare your test data\n",
    "test_data = [\n",
    "    {'input': torch.tensor([1, 2, 3]), 'target': torch.tensor([2, 3, 4])},\n",
    "    # Add more test samples here\n",
    "]\n",
    "\n",
    "# Create the test dataset\n",
    "test_dataset = YourTestDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e585350-8817-426e-9d4e-878683415d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the test_loader\n",
    "batch_size = 32  # Adjust as needed\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "991c30f7-4801-4929-a424-fb63599b6ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on test set: 4153.37\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def calculate_perplexity(model, test_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs.view(-1, outputs.size(-1)), targets.view(-1), reduction='sum')\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_tokens += targets.numel()\n",
    "    \n",
    "    perplexity = math.exp(total_loss / total_tokens)\n",
    "    return perplexity\n",
    "\n",
    "    model = TransformerModel(vocab_size, d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
    "    model.to(device)\n",
    "\n",
    "# Usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "perplexity = calculate_perplexity(model, test_loader, device)\n",
    "print(f\"Perplexity on test set: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe422ed-0527-43ee-8352-b2563be0c458",
   "metadata": {},
   "source": [
    "# BLEU Score Calculation\n",
    "For tasks involving text generation, you can use the BLEU score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53b8df1c-069a-4d85-8996-e656b4a72780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/anaconda3/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def calculate_bleu(references, hypotheses):\n",
    "    return corpus_bleu([[ref.split()] for ref in references], [hyp.split() for hyp in hypotheses])\n",
    "\n",
    "# Usage\n",
    "references = [\"this is a test\", \"another test sentence\"]\n",
    "hypotheses = [\"this is test\", \"another sentence for testing\"]\n",
    "bleu_score = calculate_bleu(references, hypotheses)\n",
    "print(f\"BLEU score: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2755e67-2f2c-4807-b7b5-38aa55790d1a",
   "metadata": {},
   "source": [
    "# ROUGE Score Calculation\n",
    "For summarization tasks, ROUGE score is commonly used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "abb54739-ab5d-482c-b429-2a56067533b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE scores: {'rouge1': {'precision': 0.7333333333333334, 'recall': 0.7333333333333334, 'fmeasure': 0.7333333333333334}, 'rouge2': {'precision': 0.25, 'recall': 0.25, 'fmeasure': 0.25}, 'rougeL': {'precision': 0.7333333333333334, 'recall': 0.7333333333333334, 'fmeasure': 0.7333333333333334}}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def calculate_rouge(references, hypotheses):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = {}\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        score = scorer.score(ref, hyp)\n",
    "        for key, value in score.items():\n",
    "            if key not in scores:\n",
    "                scores[key] = {'precision': [], 'recall': [], 'fmeasure': []}\n",
    "            scores[key]['precision'].append(value.precision)\n",
    "            scores[key]['recall'].append(value.recall)\n",
    "            scores[key]['fmeasure'].append(value.fmeasure)\n",
    "    \n",
    "    # Calculate average scores\n",
    "    avg_scores = {}\n",
    "    for key, value in scores.items():\n",
    "        avg_scores[key] = {\n",
    "            'precision': sum(value['precision']) / len(value['precision']),\n",
    "            'recall': sum(value['recall']) / len(value['recall']),\n",
    "            'fmeasure': sum(value['fmeasure']) / len(value['fmeasure'])\n",
    "        }\n",
    "    return avg_scores\n",
    "\n",
    "# Usage\n",
    "references = [\"This is the reference summary.\", \"Another reference summary.\"]\n",
    "hypotheses = [\"This is the generated summary.\", \"Another generated summary.\"]\n",
    "rouge_scores = calculate_rouge(references, hypotheses)\n",
    "print(\"ROUGE scores:\", rouge_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6092adf8-dc4f-4c22-9521-30603f12c6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
